= PostgreSQL Operator Documentation
:toc:
v2.6, {docdate}

== Overview

This document describes how to build from source code the
Postgres Operator.  If you don't want to build the images
from source, you can download them from the following:

 * Dockerhub (crunchydata/lspvc and crunchydata/postgres-operator images)
 * link:https://github.com/CrunchyData/postgres-operator/releases[Github Releases]  (pgo client and client configuration files, extracted to your $HOME)

Further details can be found in the link:design.asciidoc[PostgreSQL Operator Design] document on
how the operator is built and how it operates.

For most users of the Operator who simply want to deploy
it on a running Kubernetes cluster will find the link:https://github.com/CrunchyData/postgres-operator/blob/master/examples/quickstart.sh[Quickstart]
script useful.

Executing that script will deploy the Operator to your Kube
cluster and also set up your local user environment with the
*pgo* client in about 10 seconds.  More details on the Quickstart
script is located in the <<Quickstart>> section in this document.


== Requirements

=== Prerequisites

These versions of Kubernetes and OpenShift are required due to the use of CustomResourceDefinitions which first emerged in
these versions.

* *Kubernetes 1.7.0+*
* *OpenShift Origin 1.7.0+*

The operator is developed with the Golang versions great than or equal to version 1.8  See
link:https://golang.org/dl/[Golang website] for details on installing golang.

Pre-compiled versions of the Operator *pgo* client are provided for the x86_64, Mac OSX, and Windows hosts.

As of version 2.0, the Operator uses the following Crunchy containers (+1.6.0):

* link:https://hub.docker.com/r/crunchydata/crunchy-postgres/[PostgreSQL 9.6+ Container]
* link:https://hub.docker.com/r/crunchydata/crunchy-backup/[PostgreSQL Backup Container]
* link:https://hub.docker.com/r/crunchydata/crunchy-upgrade/[PostgreSQL Upgrade Container]
* link:https://hub.docker.com/r/crunchydata/crunchy-collect/[PostgreSQL Metrics Collection Container]

The Operator project builds and operates with the following containers:

* link:https://hub.docker.com/r/crunchydata/pgo-lspvc/[PVC Listing Container]
* link:https://hub.docker.com/r/crunchydata/pgo-rmdata/[Remove Data Container]
* link:https://hub.docker.com/r/crunchydata/postgres-operator/[postgres-operator Container]
* link:https://hub.docker.com/r/crunchydata/pgo-apiserver/[apiserver Container]
* link:https://hub.docker.com/r/crunchydata/pgo-load/[file load Container]

This Operator is developed and tested on the following operating systems:

* *CentOS 7*
* *RHEL 7*

=== Kubernetes Environment

To test the *postgres-operator*, it is required to have a Kubernetes cluster
environment.  The Operator is tested on Kubeadm Kubernetes installed clusters.  Other Kubernetes installation methods have been known to work as well.

link:https://kubernetes.io/docs/setup/independent/install-kubeadm/[Installing kubeadm - Official Kubernetes Documentation]

== Installation

=== Create Project and Clone

In your .bashrc file, include the following:
....
export GOPATH=$HOME/odev
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
export CO_BASEOS=centos7
export CO_VERSION=2.6
export CO_IMAGE_PREFIX=crunchydata
export CO_IMAGE_TAG=$CO_BASEOS-$CO_VERSION
export CO_CMD=kubectl
export CO_APISERVER_URL=https://postgres-operator:8443
export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key
....

It will be necessary to refresh your .bashrc file in order for the changes to take
effect.

....
source ~/.bashrc
....

The value of CO_IMAGE_PREFIX is used to prefix the Operator
Docker images.  For example, set this environment variable
to where you have your Operator images loaded either locally (crunchydata)
or a private Docker registry (e.g. kubeadm-master:5000/crunchydata).

The value of CO_APISERVER_URL is used by the *pgo* client to connect
to the postgres-operator *apiserver*.  This URL should include
either a DNS name for the postgres-operator service or it's Service
IP address.

Next, set up a project directory structure and pull down the project:
....
mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg
mkdir -p $GOPATH/src/github.com/crunchydata/
cd $GOPATH/src/github.com/crunchydata
git clone https://github.com/CrunchyData/postgres-operator.git
cd postgres-operator
git checkout 2.6
....

At this point, you can choose one of three options to install the postgres-operator
itself:

* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/operator-docs.asciidoc#get-prebuilt-images[Get Pre-built Images]
* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/operator-docs.asciidoc#build-from-source[Build from source]
* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/operator-docs.asciidoc#Quickstart[Quickstart]

=== Pull Postgres Containers

The Operator works with the Crunchy Container Suite
containers, you can pre-pull them as follows:

For PostgreSQL version 10:
....
docker pull crunchydata/crunchy-postgres:centos7-10.3-1.8.1
docker pull crunchydata/crunchy-backup:centos7-10.3-1.8.1
docker pull crunchydata/crunchy-upgrade:centos7-10.3-1.8.1
....

For PostgreSQL version 9.6:
....
docker pull crunchydata/crunchy-postgres:centos7-9.6.8-1.8.1
docker pull crunchydata/crunchy-backup:centos7-9.6.8-1.8.1
docker pull crunchydata/crunchy-upgrade:centos7-9.6.8-1.8.1
....

=== Get Prebuilt Images

At this point if you want to avoid building the images and binary
from source, you can pull down the Docker images as follows:
....
docker pull crunchydata/pgo-lspvc:centos7-2.6
docker pull crunchydata/pgo-load:centos7-2.6
docker pull crunchydata/pgo-rmdata:centos7-2.6
docker pull crunchydata/postgres-operator:centos7-2.6
docker pull crunchydata/pgo-apiserver:centos7-2.6
....

Next get the *pgo* client, go to the Releases page and download the tar ball, uncompress it into your $HOME directory:
....
cd $HOME
wget https://github.com/CrunchyData/postgres-operator/releases/download/2.6/postgres-operator.2.6.tar.gz
tar xvzf ./postgres-operator.2.6.tar.gz
....

Lastly, add the *pgo* client into your PATH.

You are now ready to Deploy the operator to your Kubernetes system.

=== Build from Source

Install a golang compiler, this can be done with either
your package manager or by following directions
from https://golang.org/dl/.  The operator is currently built
using golang version 1.8.X but also runs using golang version 1.9.X

Then install the project library dependencies, the godep dependency manager is used
as follows:
....
cd $COROOT
make setup
....

==== Compiling the PostgreSQL Operator
....
cd $COROOT
make all
which pgo
....

=== Create Namespace

This example is based on a kubeadm installation with the admin
user being already created. The example below assumes the cluster name is *kubernetes* and the cluster default user is *kubernetes-admin*.
....
kubectl create -f $COROOT/examples/demo-namespace.json
kubectl get namespaces
....
then set your context to the new demo namespace
....
sudo chmod o+w /etc/kubernetes
sudo chmod o+w /etc/kubernetes/admin.conf
kubectl config set-context demo --namespace=demo --cluster=kubernetes --user=kubernetes-admin
kubectl config use-context demo
kubectl config current-context
....

Permissions are granted to the Operator by means of a
Service Account called *postgres-operator*.  That service
account is added to the Operator deployment.

The postgres-operator service account is granted cluster-admin
priviledges using a cluster role binding *postgres-operator-cluster-role-binding*.

See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more
details on how to enable RBAC roles and modify the scope of the permissions
to suit your needs.

The sample service account and cluster role bindings specify
the *demo* namespace.  Edit the yaml definitions of these to match
the namespace you are deploying the operator into.

If you are not using the *demo* namespace, you will edit the following:

 * $COROOT/deploy/service-account.yaml
 * $COROOT/deploy/cluster-role-binding.yaml

=== Openshift Container Platform

To run the Operator on Openshift Container Platform note the following:

 * Openshift Container Platform 3.7 or greater is required since the Operator is based on Custom Resource Definitions which were first supported in OCP starting with version 3.7
 * the OC_CMD environment variable should be set to *oc* when operating in an Openshift environment

=== Configure Persistent Storage

The default Operator configuration is defined to use a HostPath
persistence configuration.

There are example scripts provided that will create PV and PVC resources
that can be used in your testing.

These example scripts can create sample HostPath and NFS volumes.

To create sample HostPath Persistent Volumes and Claims use the following scripts:
....
cd $COROOT/pv
./create-pv.sh
....

=== Security
==== Configure Basic Authentication

Starting in Operator version 2.3, Basic Authentication is required by the *apiserver*.
You will configure the *pgo* client to specify a basic authentication
username and password by creating a file in the user's home
directory named *.pgouser* that looks similar to this, containing only a single line:
....
testuser:testpass
....

This example specifies a username of *testuser* and a password of
*testpass*.  These values will be read by the *pgo* client and passed
to the *apiserver* on each REST API call.

For the *apiserver*, a list of usernames and passwords is
specified in the *apiserver-conf-secret* Secret.  The values specified
in a deployment are found in the following location:
....
$COROOT/conf/apiserver/pgouser
....

The sample configuration for *pgouser* is as follows:
....
username:password
testuser:testpass
....

Modify these values to be unique to your environment.

If the username and password passed by clients to the *apiserver* do
not match, the REST call will fail and a log message will be produced
in the *apiserver* container log.  The client will receive a 401 http
status code if they are not able to authenticate.

If the *pgouser* file is not found in the home directory of the *pgo* user
then the next searched location is */etc/pgo/pgouser*, and if not found
there then lastly the *PGOUSER* environment variable is searched for
a path to the basic authentication file.

You can turn off Basic Authentication entirely if you set
the BasicAuth setting in the pgo.yaml configuration file to false.

==== Configure TLS

As of Operator 2.3, TLS is used to secure communications to
the *apiserver*.  Sample keys/certs used by TLS are found
here:
....
$COROOT/conf/apiserver/server.crt
$COROOT/conf/apiserver/server.key
....

If you want to generate your own keys, you can use the script found in:
....
$COROOT/bin/make-certs.sh
....

The *pgo* client is required to use keys to connect to the *apiserver*.
Specify the keys to *pgo* by setting the following environment
variables:
....
export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key
....

The sample server keys are used as the client keys, adjust to suit
your requirements.

For the *apiserver* TLS configuration, the keys are included
in the *apiserver-conf-secret* Secret when the *apiserver* is deployed.
See the $COROOT/deploy/deploy.sh script which is where the
configMap is created.

The *apiserver* listens on port 8443 (e.g. https://postgres-operator:8443).

You can set *InsecureSkipVerify* to true if you set the NO_TLS_VERIFY
environment variable in the *deployment.json* file to *true*.  By default
this value is set to *false* if you do not specify a value.

==== RBAC

===== Kube RBAC
The *apiserver* and *postgres-operator* containers access
Kube resources and need priviledges for interacting with Kube.
The *rbac.yaml* file includes a set of roles and bindings
that allow the operator to work.  These are fine grained
controls that you can adjust to your local Kube cluster depending
on your security requirements

The *rbac.yaml* file gets executed when you deploy the operator
to your Kube cluster.

===== pgo RBAC
The *pgo* command line utility talks to the *apiserver* REST API
instead of the Kube API.  Therefore it requires its own RBAC
configuration.

Starting in Release 2.6, the */conf/apiserver/pgorole* is used to
define some sample pgo roles, *pgadmin* and *pgoreader*.

These roles are meant as samples that you can configure to suite
your own security requirements.  The *pgadmin* role grants a user
authorization to all pgo commands.  The *pgoreader* only grants
access to pgo commands that display information such as *pgo show cluster*.

The *pgorole* file is read at start up time when the operator is deployed
to your Kube cluster.

Also, the *pguser* file now includes the role that is assigned to
a specific user as follows:
....
username:password:pgoadmin
testuser:testpass:pgoadmin
readonlyuser:testpass:pgoreader
....

The following list shows the current *pgo* permissions:
.pgo Permissions
[width="60%",frame="topbot",options="header,footer"]
|======================
|Permission | Description
|ShowCluster   | allow *pgo show cluster*
|CreateCluster | allow *pgo create cluster*
|TestCluster   | allow *pgo test mycluster*
|ShowBackup    | allow *pgo show backup*
|CreateBackup  | allow *pgo backup mycluster*
|DeleteBackup  | allow *pgo delete backup mycluster*
|Label         | allow *pgo label*
|Load          | allow *pgo load*
|CreatePolicy  | allow *pgo create policy*
|DeletePolicy  | allow *pgo delete policy*
|ShowPolicy    | allow *pgo show policy*
|ApplyPolicy   | allow *pgo apply policy*
|ShowPVC       | allow *pgo show pvc*
|CreateUpgrade | allow *pgo upgrade*
|ShowUpgrade   | allow *pgo show upgrade*
|DeleteUpgrade | allow *pgo delete upgrade*
|CreateUser    | allow *pgo create user*
|User          | allow *pgo user*
|Version       | allow *pgo version*
|======================

If you are not authorized for a *pgo* command the user will
get back this response:
....
....



=== Configuration

The *apiserver* uses the following configuration files found in $COROOT/conf/apiserver to determine how the Operator will provision PostgreSQL containers:
....
$COROOT/conf/apiserver/pgo.yaml
$COROOT/conf/apiserver/pgo.lspvc-template.json
$COROOT/conf/apiserver/pgo.load-template.json
....

Note that the default *pgo.yaml* file assumes you are going to use *HostPath* Persistent Volumes for
your storage configuration.  Adjust this file for NFS or other storage configurations.

The version of PostgreSQL container the Operator will deploy is determined
by the *CCPImageTag* setting in the *$COROOT/conf/apiserver/pgo.yaml*
configuration file.  By default, this value is set to the latest
release of the Crunchy Container Suite.

More in-depth explanations of postgres operator configurations are available below.

=== Deploy the PostgreSQL Operator
*NOTE*: This will create and use */data* on your
local system as the persistent store for the operator to use
for its persistent volume.
....
cd $COROOT
make deployoperator
kubectl get pod -l 'name=postgres-operator'
....

You should see output similar to:
....
NAME                                 READY     STATUS    RESTARTS   AGE
postgres-operator-7f8db87c7b-4tk52   2/2       Running   0          8s
....

This output shows that both the *apiserver* and *postgres-operator* containers
are in ready state and the pod is running.

You can find the operator service IP address as follows:
....
kubectl get service postgres-operator
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
postgres-operator   ClusterIP   10.105.56.167   <none>        8080/TCP,8443/TCP   1m
....

In this example, the *apiserver* is reachable at *https://10.105.56.167:8443*.


When you first run the operator, it will create the required
CustomResourceDefinitions. You can view these as follows:

....
kubectl get crd
....

Instead of using the bash script you can also deploy the operator using the provided Helm chart:
....
cd $COROOT/chart
helm install ./postgres-operator
helm ls
....

=== Verify Installation

When you first run the operator, it will look for the presence of the
predefined custom resource definitions, and create them if not found.
The best way to verify a successful deployment of the Operator is by
viewing these custom resource definitions:

....
kubectl get crd
kubectl get pgclusters
kubectl get pgreplicas
kubectl get pgbackups
kubectl get pgupgrades
kubectl get pgpolicies
kubectl get pgingests
kubectl get pgtasks
....

At this point, you should be ready to start using the *pgo* client!  Be
sure to set the environment variable *CO_APISERVER_URL* to the DNS
name of the *postgres-operator* service or to the IP address of the
*postgres-operator* service IP address.  For example:

....
export CO_APISERVER_URL=https://10.105.56.167:8443
....

Or if you have DNS configured on your client host:
....
export CO_APISERVER_URL=https://postgres-operator.demo.svc.cluster.local:8443
....


== Performing a Smoke Test

A simple *smoke test* of the postgres operator includes testing
the following:

 * get version information (*pgo version*)
 * create a cluster (*pgo create cluster testcluster*)
 * scale a cluster (*pgo scale testcluster *)
 * show a cluster (*pgo show cluster testcluster*)
 * show all clusters (*pgo show cluster all*)
 * backup a cluster (*pgo backup testcluster*)
 * show backup of cluster (*pgo show backup testcluster*)
 * show backup pvc of cluster (*pgo show pvc testcluster-backup-pvc*)
 * restore a cluster (*pgo create cluster restoredcluster --backup-pvc=testcluster-backup-pvc --backup-path=testcluster-backups/2017-01-01-01-01-01 --secret-from=testcluster*)
 * test a cluster (*pgo test restoredcluster*)
 * minor upgrade a cluster (*pgo upgrade testcluster*)
 * major upgrade a cluster (*pgo upgrade testcluster --upgrade-type=major*)
 * delete a cluster (*pgo delete cluster testcluster --delete-data --delete-backups*)
 * create a policy from local file (*pgo create policy policy1 --in-file=./examples/policy/policy1.sql*)
 * create a policy from git repo (*pgo create policy gitpolicy --url=https://github.com/CrunchyData/postgres-operator/blob/master/examples/policy/gitpolicy.sql*)
 * repeat testing using emptydir storage type
 * repeat testing using create storage type
 * repeat testing using existing storage type
 * create a series of clusters  (*pgo create cluster myseries --series=2*)
 * apply labels at cluster creation (*pgo create cluster xraydb --series=2 --labels=project=xray*)
 * apply a label to an existing set of clusters (*pgo label --label=env=research --selector=project=xray*)
 * create a user for a given cluster (*pgo create user user0 --valid-days=30 --managed --db=userdb --selector=name=xraydb0*)
 * load a csv file into a cluster (*pgo load --load-config=./sample-load-config.json --selector=project=xray*)
 * extend a user's password allowed age (*pgo user --change-password=user0 --valid-days=10 --selector=name=xraydb1*)
 * drop user access (*pgo user --delete-user=user2 --selector=project=xray*)
 * check password age (*pgo user --expired=10 --selector=project=xray*)
 * backup an entire project (*pgo backup --selector=project=xray*)
 * delete an entire project (*pgo delete cluster --selector=project=xray*)
 * create a cluster with a crunchy-collect sidecar(*pgo create cluster testcluster --metrics*)

More detailed explanations of the commands can be found below <<pgo Commands>>.

== Makefile Targets

The following table describes the Makefile targets:
.Makefile Targets
[width="80%",frame="topbot",options="header,footer"]
|======================
|Target | Description
|all        | compile all binaries and build all images
|setup        | fetch the dependent packages required to build with
|deployoperator        | deploy the Operator (apiserver and postgers-operator) to Kubernetes
|main        | compile the postgres-operator
|runmain        | locally execute the postgres-operator
|pgo        | build the pgo binary
|runpgo        | run the pgo binary
|runapiserver        | run the apiserver binary outside of Kube
|clean        | remove binaries and compiled packages, restore dependencies
|operatorimage        | compile and build the postgres-operator Docker image
|apiserverimage        | compile and build the apiserver Docker image
|lsimage        | build the lspvc Docker image
|loadimage        | build the file load Docker image
|rmdataimage        | build the data deletion Docker image
|release        | build the postgres-operator release
|======================


== Operator Configuration

This document describes the configuration options
for the *PostgreSQL operator*.

=== pgo Client Configuration

Starting with Operator version 2.1, the *pgo.yaml* configuration
file is used solely by the *apiserver* and has no effect on the *pgo* client.  With this change, the Operator configuration is centralized to
the *apiserver* container which is deployed alongside the *postgres-operator* container.

Sample Operator configuration files for various storage configurations are located in the $COROOT/examples directory.

To configure the Operator, modify the settings found in
*$COROOT/conf/apiserver/pgo.yaml* to meet your project needs.  Typically
you will modify the storage and namespace settings.

==== pgo Configuration Format

The default pgo configuration file, included in
*$COROOT/conf/apiserver/pgo.yaml*, looks like this:

[source,yaml]
....
BasicAuth:  true
Cluster:
  CCPImageTag:  centos7-10.3-1.8.1
  Port:  5432
  User:  testuser
  Database:  userdb
  PasswordAgeDays:  60
  PasswordLength:  8
  Strategy:  1
  Replicas:  0
PrimaryStorage: storage1
BackupStorage: storage1
ReplicaStorage: storage1
Storage:
  storage1:
    AccessMode:  ReadWriteMany
    Size:  200M
    StorageType:  create
  storage2:
    AccessMode:  ReadWriteMany
    Size:  333M
    StorageType:  create
  storage3:
    AccessMode:  ReadWriteMany
    Size:  440M
    StorageType:  create
DefaultContainerResource: small
ContainerResources:
  small:
    RequestsMemory:  2Gi
    RequestsCPU:  0.5
    LimitsMemory:  2Gi
    LimitsCPU:  1.0
  large:
    RequestsMemory:  8Gi
    RequestsCPU:  2.0
    LimitsMemory:  12Gi
    LimitsCPU:  4.0
Pgo:
  Audit:  false
  Metrics:  false
  LSPVCTemplate:  /config/pgo.lspvc-template.json
  CSVLoadTemplate:  /config/pgo.load-template.json
  COImagePrefix:  crunchydata
  COImageTag:  centos7-2.6
....

Values in the pgo configuration file have the following meaning:

.pgo Configuration File Definitions
[width="90%",cols="m,2",frame="topbot",options="header"]
|======================
|Setting | Definition
|BasicAuth        | if set to *true* will enable Basic Authentication
|Cluster.CCPImageTag        |newly created containers will be based on this image version (e.g. centos7-10.3-1.8.1), unless you override it using the --ccp-image-tag command line flag
|Cluster.Port        | the PostgreSQL port to use for new containers (e.g. 5432)
|Cluster.User        | the PostgreSQL normal user name
|Cluster.Strategy        | sets the deployment strategy to be used for deploying a cluster, currently there is only strategy *1*
|Cluster.Replicas        | the number of cluster replicas to create for newly created clusters
|Cluster.Policies        | optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog
|Cluster.PasswordAgeDays        | optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 60 days
|Cluster.PasswordLength        | optional, if set, will determine the password length used when creating passwords, defaults to 8
|PrimaryStorage    |required, the value of the storage configuration to use for the primary PostgreSQL deployment
|BackupStorage    |required, the value of the storage configuration to use for backups
|ReplicaStorage    |required, the value of the storage configuration to use for the replica PostgreSQL deployments
|Storage.storage1.StorageClass        |for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)
|Storage.storage1.AccessMode        |the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.
|Storage.storage1.Size        |the size to use when creating new PVCs (e.g. 100M, 1Gi)
|Storage.storage1.StorageType        |supported values are either *dynamic*, *existing*, *create*, or *emptydir*, if not supplied, *emptydir* is used
|Storage.storage1.Fsgroup        | optional, if set, will cause a *SecurityContext* and *fsGroup* attributes to be added to generated Pod and Deployment definitions
|Storage.storage1.SupplementalGroups        | optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions
|DefaultContainerResource    |optional, the value of the container resources configuration to use for all database containers, if not set, no resource limits or requests are added on the database container
|ContainerResources.small.RequestsMemory        | request size of memory in bytes
|ContainerResources.small.RequestsCPU        | request size of CPU cores
|ContainerResources.small.LimitsMemory        | request size of memory in bytes
|ContainerResources.small.LimitsCPU        | request size of CPU cores
|ContainerResources.large.RequestsMemory        | request size of memory in bytes
|ContainerResources.large.RequestsCPU        | request size of CPU cores
|ContainerResources.large.LimitsMemory        | request size of memory in bytes
|ContainerResources.large.LimitsCPU        | request size of CPU cores
|Pgo.LSPVCTemplate        | the PVC lspvc template file that lists PVC contents
|Pgo.LoadTemplate        | the load template file used for load jobs
|Pgo.COImagePrefix        | image tag prefix to use for the Operator containers
|Pgo.COImageTag        | image tag to use for the Operator containers
|Pgo.Audit        | boolean, if set to true will cause each apiserver call to be logged with an *audit* marking
|Pgo.Metrics        | boolean, if set to true will cause each new cluster to include crunchy-collect as a sidecar container for metrics collection, if set to false (default), users can still add metrics on a cluster-by-cluster basis using the pgo command flag --metrics
|======================

=== Storage Configurations

Starting with release 2.5, you can now define n-number of Storage configurations
within the *pgo.yaml* file.  Those Storage configurations follow these conventions:

 * they must have lowercase name (e.g. storage1)
 * they must be unique names (e.g. mydrstorage, faststorage, slowstorage)

These Storage configurations are referenced in the BcakupStorage, ReplicaStorage,
and PrimaryStorage configuration values.  However, there are command line
options in the *pgo* client that will let a user override these default global
values to offer you the user a way to specify very targeted storage configurations
when needed (e.g. disaster recovery storage for certain backups).

You can set the storage AccessMode values to the following:

* *ReadWriteMany* - mounts the volume as read-write by many nodes
* *ReadWriteOnce* - mounts the PVC as read-write by a single node
* *ReadOnlyMany* - mounts the PVC as read-only by many nodes

These Storage configurations are validated when the *pgo-apiserver* starts, if a
non-valid configuration is found, the apiserver will abort.  These Storage values are
only read at apiserver start time.

=== Overriding Container Resources Configuration Defaults

In the *pgo.yaml* configuration file you have the option
to configure a default container resources configuration
that when set will add CPU and memory resource limits and requests
values into each database container when the container is created.

You can also override the default value using the *--resources-config* 
command flag when creating a new cluster:
....
pgo create cluster testcluster --resources-config=large
....

Note, if you try to allocate more resources than your
host or Kube cluster has available then you will see your
pods wait in a *Pending* status.   The output from a *kubectl describe pod*
command will show output like this in this case:
....
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  49s (x8 over 1m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient memory (1).
....

=== Overriding Storage Configuration Defaults

....
pgo create cluster testcluster --storage-config=bigdisk
....

That example will create a cluster and specify a storage configuration
of *bigdisk* to be used for the primary database storage, the replica
storage will default to the value of ReplicaStorage as specified
in *pgo.yaml*.

....
pgo create cluster testcluster2 --storage-config=fastdisk --replica-storage-config=slowdisk
....

That example will create a cluster and specify a storage configuration
of *fastdisk* to be used for the primary database storage, the replica
storage will use the storage configuration *slowdisk*.

....
pgo backup testcluster --storage-config=offsitestorage
....

That example will create a backup and use the *offsitestorage*
storage configuration for persisting the backup.

=== Disaster Recovery Using Storage Configurations

A simple support for disaster recovery can be obtained
by leveraging network storage, Kubernetes storage classes, and
the storage configuration options within the Operator.

For example, if you define a Kubernetes storage class that refers
to a storage backend that is running within your disaster recovery
site, and then use that storage class as a storage configuration
for your backups, you essentially have moved your backup files
automatically to your DR site thanks to network storage.

image::Operator-DR-Storage.png?raw=true[]

=== Operator Configuration (Server)

The operator is run as a Kubernetes Deployment on the Kubernetes cluster
within a namespace.

Execute the Makefile target *deployoperator* to deploy the Operator.

You can also create NFS PV(s) using the create-pv-nfs.sh script.

To enable *debug* level messages from the operator pod, set the *CRUNCHY_DEBUG* environment variable to *true* within its deployment file *deployment.json*.

==== Operator Templates

The database and cluster Kubernetes objects that get created by the operator
are based on json templates that are added into the operator deployment
by means of a mounted volume.

The templates are located in the *$COROOT/conf/postgres-operator* directory
and get added into a config map which is mounted by the operator deployment.

==== Persistence

Different ways of handling storage are specified by a user in
the *.pgo.yaml* configuration file by specifying values within
the ReplicaStorage, PrimaryStorage, and BackupStorage settings.

The following StorageType values are possible:

 * *dynamic* - this will allow for dynamic provisioning of storage using a StorageClass.
 * *existing* - This setting allows you to use a PVC that already exists.
 For example, if you have a NFS volume mounted to a PVC, all PostgreSQL clusters
 can write to that NFS volume mount via a common PVC. When set, the Name
 setting is used for the PVC.
 * *create* - This setting allows for the creation of a new PVC for
 each PostgreSQL cluster using a naming convention of *clustername*-pvc*.
 When set, the *Size*, *AccessMode* settings are used in
 constructing the new PVC.
 * *emptydir* - If a StorageType value is not defined, *emptydir* is used by default.
 This is a volume type that’s created when a pod is assigned to a node and exists as
 long as that pod remains running on that node; it is deleted as soon as the pod is
 manually deleted or removed from the node.

The operator will create new PVCs using this naming convention:
*dbname-pvc* where *dbname* is the database name you have specified.  For
example, if you run:
....
pgo create cluster example1
....

It will result in a PVC being created named *example1-pvc* and in
the case of a backup job, the pvc is named *example1-backup-pvc*

There are currently 3 sample pgo configuration files provided
for users to use as a starting configuration:

 * pgo.yaml.emptydir - this configuration specifies *emptydir* storage
 to be used for databases
 * pgo.yaml.nfs - this configuration specifies *create* storage to
 be used, this is used for NFS storage for example where you want to
 have a unique PVC created for each database
 * pgo.yaml.dynamic - this configuration specifies *dynamic* storage
 to be used, namely a *storageclass* that refers to a dynamic provisioning
 strorage such as StorageOS or Portworx, or GCE.

== pgo Commands

Prior to using *pgo*, users will need to specify the
*postgres-operator* URL as follows:
....
kubectl get service postgres-operator
NAME                CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
postgres-operator   10.104.47.110   <none>        8443/TCP   7m
export CO_APISERVER_URL=https://10.104.47.110:8443
pgo version
....

=== pgo version

To see what version of pgo client and postgres-operator you are
running, use the following:
....
pgo version
....

=== pgo create cluster

To create a database, use the following:
....
pgo create cluster mycluster
....

A more complex example is to create a *series* of clusters such
as:
....
pgo create cluster xraydb --series=3 --labels=project=xray --policies=xrayapp,rlspolicy
....

In the example above, we provision 3 clusters that have a number appended
into their resulting cluster name, apply a user defined label to each
cluster, and also apply user defined policies to each cluster after
they are created.

You can then view that database as:
....
pgo show cluster mydatabase
....

Also, if you like to see JSON formatted output, add the *-o json* flag:
....
pgo show cluster mydatabase -o json
....

The output will give you the current status of the database pod
and the IP address of the database service.  If you have *postgresql*
installed on your test system you can connect to the
database using the service IP address:
....
psql -h 10.105.121.12 -U postgres postgres
....

More details are available on user management below, however, you may wish to take note
that user credentials are created in the file $COROOT/deploy/create-secrets.sh upon
deployment of the Operator. The following user accounts and passwords are created by
default for connecting to the PostgreSQL clusters:

*username*: postgres
*password*: password

*username*: primaryuser
*password*: password

*username*: testuser
*password*: password

You can view *all* databases using the special keyword *all*:
....
pgo show cluster all
....

You can filter the results based on the Postgres Version:
....
pgo show cluster all --version=9.6.2
....

You can also add metrics collection to a cluster by using the *--metrics*
command flag as follows:
....
pgo create cluster testcluster --metrics
....

This command flag causes a *crunchy-collect* container to be added to the
database cluster pod and enables metrics collection on that database pod.
For this to work, you will need to configure the Crunchy metrics
example as found in the Crunchy Container Suite.

New clusters typically pick up the container image version to use
based on the *pgo* configuration file's CCP_IMAGE_TAG setting.  You
can override this value using the *--ccp-image-tag* command line
flag:
....
pgo create cluster mycluster --ccp-image-tag=centos7-9.6.5-1.6.0
....

You can also add a pgpool deployment into a cluster by using the *--pgpool*
command flag as follows:
....
pgo create cluster testcluster --pgpool
....

This will cause a *crunchy-pgpool* container to be started and initially
configured for a cluster and the *testuser* cluster credential.  See
below for more details on running a pgpool deployment as part of
your cluster.

=== pgo backup

You can start a backup job for a cluster as follows:
....
pgo backup mycluster
....

You can view the backup:
....
pgo show backup mycluster
....

View the PVC folder and the backups contained therein:

....
pgo show pvc mycluster-backup-pvc
pgo show pvc mycluster-backup-pvc --pvc-root=mycluster-backups
....

The output from this command is important in that it can let you
copy/paste a backup snapshot path and use it for restoring a database
or essentially cloning a database with an existing backup archive.

For example, to restore a database from a backup archive:
....
pgo create cluster restoredb --backup-path=mycluster-backups/2017-03-27-13-56-49 --backup-pvc=mycluster-pvc --secret-from=mycluster
....

This will create a new database called *restoredb* based on the
backup found in *mycluster-backups/2017-03-27-13-56-49* and the
secrets of the *mycluster* cluster.

Selectors can be used to perform backups as well, for example:
....
pgo backup  --selector=project=xray
....

In this example, any cluster that matches the selector will cause
a backup job to be created.

When you request a backup, *pgo* will prompt you if you want
to proceed because this action will delete any existing backup job
for this cluster that might exist.  The backup files will still 
be left intact but the actual Kubernetes Job will be removed prior
to creating a new Job with the same name.

=== pgo delete backup

To delete a backup enter the following:
....
pgo delete backup mybackup
....

=== pgo delete cluster

You can remove a cluster by running:
....
pgo delete cluster restoredb
....

Note, that this command will not remove the PVC associated with
this cluster.

Selectors also apply to the delete command as follows:
....
pgo delete cluster  --selector=project=xray
....

This command will cause any cluster matching the selector
to be removed.

You can remove a cluster and it's data files by running:
....
pgo delete cluster restoredb --delete-data
....

You can remove a cluster, it's data files, and all backups by running:
....
pgo delete cluster restoredb --delete-data --delete-backups
....

When you specify a destructive delete like above, you will be prompted
to make sure this is what you want to do.  If you don't want to
be prompted you can enter the *--no-prompt* command line flag.


=== pgo scale

When you create a Cluster, you will see in the output a variety of Kubernetes objects were created including:

 * a Deployment holding the primary PostgreSQL database
 * a Deployment holding the replica PostgreSQL database
 * a service for the primary database
 * a service for the replica databases

Since Postgres is a single-primary database by design, the primary
Deployment is set to a replica count of 1, it can not scale beyond 1.

With Postgres, you can any n-number of replicas each of which
connect to the primary forming a streaming replication postgres cluster.
The Postgres replicas are read-only, whereas the primary is read-write.
To create a Postgres replica enter a command such as:
....
pgo scale mycluster 
....

The *pgo scale* command is additive, in that each time you execute
it, it will create another replica which is added to the Postgres
cluster.

There are 2 service connections available to the PostgreSQL cluster. One is
to the primary database which allows read-write SQL processing, and
the other is to the set of read-only replica databases.  The replica
service performs round-robin load balancing to the replica databases.

You can connect to the primary database and verify that it is replicating
to the replica databases as follows:
....
psql -h 10.107.180.159 -U postgres postgres -c 'table pg_stat_replication'
....

You can view *all* clusters using the special keyword *all*:
....
pgo show cluster all
....

You can filter the results by Postgres version:
....
pgo show cluster all --version=9.6.2
....


=== pgo upgrade

You can perform a minor Postgres version upgrade
of either a database or cluster as follows:
....
pgo upgrade mycluster
....

When you run this command, it will cause the operator
to delete the existing containers of the database or cluster
and recreate them using the currently defined Postgres
container image specified in your pgo configuration file.

The database data files remain untouched, only the container
is updated, this will upgrade your Postgres server version only.

You can perform a major Postgres version upgrade
of either a database or cluster as follows:
....
pgo upgrade mycluster --upgrade-type=major
....

When you run this command, it will cause the operator
to delete the existing containers of the database or cluster
and recreate them using the currently defined Postgres
container image specified in your pgo configuration file.

The database data files are converted to the new major Postgres
version as specified by the current Postgres image version
in your pgo configuration file.

In this scenario, the upgrade is performed by the Postgres
pg_upgrade utility which is containerized in the *crunchydata/crunchy-upgrade*
container.  The operator will create a Job which runs the upgrade container,
using the existing Postgres database files as input, and output
the updated database files to a new PVC.

Once the upgrade job is completed, the operator will create the
original database or cluster container mounted with the new PVC
which contains the upgraded database files.

As the upgrade is processed, the status of the *pgupgrade* CRD is
updated to give the user some insight into how the upgrade is
proceeding.  Upgrades like this can take a long time if your
database is large.  The operator creates a watch on the upgrade
job to know when and how to proceed.

Likewise, you can upgrade the cluster using a command line flag:
....
pgo upgrade mycluster --ccp-image-tag=centos7-9.6.6-1.7.0
pgo upgrade mycluster --upgrade-type=major --ccp-image-tag=centos7-9.6.6-1.7.0
....


=== pgo delete upgrade

To remove an upgrade CRD, issue the following:
....
pgo delete upgrade
....

=== pgo show pvc

You can view the files on a PVC as follows:
....
pgo show pvc mycluster-pvc
....

In this example, the PVC is *mycluster-pvc*.  This command is useful
in some cases to examine what files are on a given PVC.

In the case where you want to list a specific path on a PVC
you can specify the path option as follows:
....
pgo show pvc mycluster-pvc --pvc-root=mycluster-backups
....

You can also list all PVCs that are created by the operator
using:
....
pgo show pvc all
....


=== pgo show cluster

You can view the passwords used by the cluster as follows:
....
pgo show cluster mycluster --show-secrets=true
....

Passwords are generated if not specified in your *pgo* configuration.

=== pgo test

You can test the database connections to a cluster:
....
pgo test mycluster
....

This command will test each service defined for the cluster using
the postgres, primary, and normal user accounts defined for the
cluster.  The cluster credentials are accessed and used to test
the database connections.  The equivalent *psql* command is printed
out as connections are tried, along with the connection status.

Like other commands, you can use the selector to test a series
of clusters:
....
pgo test --selector=env=research
pgo test all
....

You can get output using the *--output* flag:
....
pgo test all -o json
....

=== pgo create policy

To create a policy use the following syntax:
....
pgo create policy policy1 --in-file=/tmp/policy1.sql
pgo create policy policy1 --url=https://someurl/policy1.sql
....

When you execute this command, it will create a policy named *policy1*
using the input file */tmp/policy1.sql* as input.  It will create
on the server a PgPolicy CRD with the name *policy1* that you can
examine as follows:

....
kubectl get pgpolicies policy1 -o json
....

Policies get automatically applied to any cluster you create if
you define in your *pgo.yaml* configuration a CLUSTER.POLICIES
value.  Policy SQL is executed as the *postgres* user.

To view policies:
....
pgo show policy all
....

=== pgo delete policy

To delete a policy use the following form:
....
pgo delete policy policy1
....

=== pgo apply

To apply an existing policy to a set of clusters, issue
a command like this:
....
pgo apply policy1 --selector=name=mycluster
....

When you execute this command, it will look up clusters that
have a label value of *name=mycluster* and then it will apply
the *policy1* label to that cluster and execute the policy
SQL against that cluster using the *postgres* user account.

WARNING:  policies are executed as the superuser in PostgreSQL therefore
take caution when using them.

If you want to view the clusters than have a specific policy applied
to them, you can use the *--selector* flag as follows to filter on a
policy name (e.g. policy1):
....
pgo show cluster --selector=policy1=pgpolicy
....


=== pgo user

To create a new Postgres user to the *mycluster* cluster, execute:
....
pgo createa user sally --selector=name=mycluster
....

To delete a Postgres user in the *mycluster* cluster, execute:
....
pgo user --delete-user=sally --selector=name=mycluster
....

To delete that user in all clusters:
....
pgo user --delete-user=sally
....

To change the password for a user in the *mycluster* cluster:
....
pgo user --change-password=sally --selector=name=mycluster
....

The password is generated and applied to the user sally.

To see user passwords that have expired past a certain number
of days in the *mycluster* cluster:
....
pgo user --expired=7 --selector=name=mycluster
....

To assign users to a cluster:
....
pgo create user user1 --valid-days=30 --managed --db=userdb --selector=name=xraydb1
....

In this example, a user named *user1* is created with a *valid until* password date set to expire in 30 days.  That user will be granted access to the *userdb* database.  This user account also will have an associated *secret* created to hold the password that was generated for this user.  Any clusters that match the selector value will have this user created on it.

To change a user password:
....
pgo user --change-password=user1 --valid-days=10 --selector=name=xray1
....

In this example, a user named *user1* has its password changed to a generated
value and the *valid until* expiration date set to 10 days from now, this
command will take effect across all clusters that match the selector.  If you
specify *valid-days=-1* it will mean the password will not expire (e.g. infinity).

To drop a user:
....
pgo user --delete-user=user3   --selector=project=xray
....

To see which passwords are set to expire in a given number of days:
....
pgo user --expired=10  --selector=project=xray
....

In this example, any clusters that match the selector are queried to see
if any users are set to expire in 10 days.

To update expired passwords in a cluster:
....
pgo user --update-passwords --selector=name=mycluster
....

=== pgo label

You can apply a user defined label to a cluster as follows:
....
pgo label --label=env=research  --selector=project=xray
....

In this example, we apply a label of *env=research* to any
clusters that have an existing label of *project=xray* applied.

=== pgo load

A CSV file loading capability is supported currently.  You can
test that by creating a SQL Policy which will create a database
table that will be loaded with the CSV data.  For example:

....
pgo create policy xrayapp --in-file=$COROOT/examples/policy/xrayapp.sql
....

Then you can load a sample CSV file into a database as follows:

....
pgo load --load-config=$COROOT/examples/sample-load-config.json  --selector=name=mycluster
....

The loading is based on a load definition found in the *sample-load-config.json* file.  In that file, the data to be loaded is specified. When the *pgo load* command is executed, Jobs will be created to perform the loading for each cluster that matches the selector filter.

If you include the *--policies* flag, any specified policies will be applied prior to the data being loaded.  For
example:
....
pgo load --policies="rlspolicy,xrayapp" --load-config=$COROOT/examples/sample-load-config.json --selector=name=mycluster
....

Likewise you can load a sample json file into a database as follows:
....
pgo load --policies=jsonload --load-config=$COROOT/examples/sample-json-load-config.json  --selector=name=mycluster
....

The load configuration file has the following YAML attributes:

.Load Configuration File Definitions
[width="90%",cols="m,2",frame="topbot",options="header"]
|======================
|COImagePrefix|  the pgo-load image prefix to use for the load job
|COImageTag|  the pgo-load image tag to use for the load job
|DbDatabase|  the database schema to use for loading the data
|DbUser|  the database user to use for loading the data
|DbPort|  the database port of the database to load
|TableToLoad|  the PostgreSQL table to load
|FilePath|  the name of the file to be loaded
|FileType|  either csv or json, determines the type of data to be loaded
|PVCName|  the name of the PVC that holds the data file to be loaded
|SecurityContext| either fsGroup or SupplementalGroup values
|======================

== bash Completion

There is a bash completion file that is included for users to try, this
is located in the repository at *example/pgo-bash-completion*. To use it:
....
cp $COROOT/example/pgo-bash-completion /etc/bash_completion.d/pgo
su - $USER
....

== Quickstart

The *quickstart-for-gke.sh* script will allow users to set up the Postgres Operator quickly on GKE including PKS.
This script is tested on GKE but can be modified for use with other Kubernetes environments as well.

The script requires a few things in order to work:

 * wget utility installed
 * kubectl utility installed

Executing the script will give you a default Operator deployment
that assumes *dynamic* storage and a storage class named *standard*,
things that GKE provides.

The script performs the following:

 * downloads the Operator configuration files
 * sets the $HOME/.pgouser file to default settings
 * deploys the Operator Deployment
 * sets your .bashrc to include the Operator environment variables
 * sets your $HOME/.bash_completion file to be the *pgo* bash_completion file

Our plans are to provide other customized versions of the Quickstart script
over time, as well as support Mac and Windows versions of the script.

== REST API

Because the *apiserver* implements a REST API, you can integrate
with it using your own application code.  To demostrate this, the
following *curl* commands show the API usage:

*pgo version*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/version
....

*pgo show policy all*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/policies/all
....

*pgo show pvc danger-pvc*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/pvc/danger-pvc
....

*pgo show cluster mycluster*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clusters/mycluster
....

*pgo show upgrade mycluster*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/upgrades/mycluster
....

*pgo test mycluster*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clusters/test/mycluster
....

*pgo show backup mycluster*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/backups/mycluster
....

== Deploying pgpool 

It is optional but you can cause a pgpool Deployment to be
created as part of a Postgres cluster.  Running pgpool only 
makes sense when you have both a primary and some number of
replicas deployed as part of your Postgres cluster.  The current
pgpool configuration deployed by the operator only works when
you have both a primary and replica running.

When a user creates the cluster they can pass a command flag as follows:
....
pgo create cluster cluster1 --pgpool
pgo scale cluster1 
....

This will cause the operator to create a Deployment that includes
the *crunchy-pgpool* container along with a replica.  That container will create
a configuration that will perform SQL routing to your
cluster services, both for the primary and replica services.  

Pgpool examines the SQL it receives and routes the SQL statement
to either the primary or replica based on the SQL action specifically
it will send writes and updates to only the *primary* service.  It
will send read-only statements to the *replica* service.

When the operator deploys the pgpool container, it creates a 
secret (e.g. mycluster-pgpool-secret) that contains pgpool
configuration files.  It fills out templated versions of these
configuration files specifically for this postgres cluster.

Part of the pgpool deployment also includes creating a *pool_passwd*
file that will allow the *testuser* credential to authenticate
to pgpool.  Adding additional users to the pgpool configuration
currently requires human intervention specifically creating
a new pgpool secret and bouncing the pgpool pod to pick up the
updated secret.  Future operator releases will attempt to 
provide *pgo* commands to let you automate the addition or removal
of a pgpool user.

Currently to update a pgpool user within the *pool_passwd* configuration
file, you will have to copy the existing files from the 
secret to your local system, update the credentials in *pool_passwd*
with your new user credentials, and then recreate the pgpool
secret, and finally restart the pgpool pod to pick up the updated
configuration files.

Example:
....
kubectl cp demo/wed10-pgpool-6cc6f6598d-wcnmf:/pgconf/ /tmp/foo
....

That command gets a running set of secret pgpool configuration
files and places them locally on your system for you to edit.

*pgpool* requires a specially formatted password credential
to be placed into *pool_passwd*.  There is a golang program
included in $COROOT/golang-examples/gen-pgpool-pass.go* that
when run will generate the value to use within the *pgpool_passwd*
configuration file.
....
go run $COROOT/golang-examples/gen-pgpool-pass.go
Enter Username: testuser
Enter Password: 
Password typed: e99Mjt1dLz
hash of password is [md59c4017667828b33762665dc4558fbd76]
....

The value *md59c4017667828b33762665dc4558fbd76* is what you will use
in the *pool_passwd* file.

Then, create the new secrets file based on those updated files:
....
$COROOT/bin/create-pgpool-secrets.sh
....

Lastly for pgpool to pick up the new secret file, delete the existing
deployment pod:
....
kubectl get deployment wed-pgpool
kubectl delete pod wed10-pgpool-6cc6f6598d-wcnmf
....

The pgpool deployment will spin up another pgpool which will pick up
the updated secret file.

